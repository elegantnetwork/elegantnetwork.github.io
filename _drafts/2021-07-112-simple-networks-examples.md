---
layout: post
comments: true
author: Justin Pietsch
title: Examples of simplifying networking (and lessons in Engineering tradeoffs)
excerpt:
description: What does it mean to simplify your network? Here are some examples and why they were important.
---

You've probably heard people say that networks are too complex and need to be made simpler. I certainly believe that. I think people build too complicated networks and don't consider how hard it will be to operate those over time. There are many reasons: people believe vendor hype, or like shiny things, or the business asks for things that require complexity. However, it doesn't really matter why. The more complex the network, the harder to operate and the harder to make changes. In the end, everything comes down to how well you can operate your network over time. Just because a network is complex doesn't mean that it's wrong. Remember, **engineering is the art of tradeoffs**, so the areas that bring complexity are worth examining to see how to make things better.

I think it might be helpful to go through examples I've experienced of simplifying networks. Often it does require a discussion with the people using the network. You must weight the complexity of operating all the features you've been asked for and ask if there is a better way of doing that. You will get different answers with different people, and different companies/cultures involved in the discussion, but the point is to do the analysis and understand the tradeoffs.

## L2/Vlans

When I started at Amazon in late 2002, we had two datacenters and they were filled with L2 and vlans. There were three main networks: website, dbnet, and corporate. Each host in the datacenter had two NICs and were attached to two of the three networks, depending on the purpose of the host. I wasn't around when these were made, I think the idea was to keep the database hosts on dbnet secure from the internet, they were on the dbnet and the corporate net, but not on the website. Website hosts were on website and dbnet but not corporate. I don't know how hosts were decided on which vlan they were on inside those networks. This is a kind of complexity that I am especially offended by; psuedo-security which doesn't really make things more secure but makes you feel like it and makes something else more complicated. However, it was not easy to chance. Just because I was offended and thought there was a better way didn't mean we could make the change, it took work and time.

In the early 2000s, Load Balancers were almost completely L2 based, at least the ones that Amazon used. Which means that the LB needed to be the default gateway for devices that needed load balancing. IIRC, if one host needed to talk to a service that had hosts on the same vlan, then they couldn't talk successfully, so that service needed to be moved to another VLAN. This made having lots of services more and more difficult, requiring the networking team to know what software was on what host. If you can avoid it, you never really want the networking team to be in the middle of software decisions.

As you can imagine, lots of outages and lots of operational nightmares. Hard to keep up and hard to scale. I think most people know to hate L2 and vlans and spanning tree these days, but that was less so in the early 2000s.

Maybe you've heard of Amazon's famous 2-pizza teams. The concept is a little overblown in that it was almost never that formal or fixed, but it's true that the culture was made so that teams could (and did) spin up as many different services that they needed to build the things that they needed for customers. This all lead to an explosion of software services, interacting with other software in ways that nobody truly understood or understands.

If we had thought ahead we would have seen that this couldn't possibly work over time. Instead I think enough of us were offended by L2, and we'd had enough operational issues we just argued to make it simpler. How could we decouple this knot of confusion? The first step was to move the L4/L7 load balancers. We needed the load balancers to rewrite the destination headers and not need to be in the middle of everything. L2 only load balancing is harder to debug. Also we had several DDoS attacks and the LBs we had were incapable of defending us from external attacks and so we brought in a new LB that did L4/L7. Getting LBs out of the direct path and no longer being the default gateway gave us a lot more flexibility. Flexibility that we wouldn't have survived without, but at the time, we didn't know how badly we'd need it. We just knew that they way we were working wasn't working.

That effectively got rid of most of the L2 requirement. Except that also each datacenter host had those two NICs that needed to be in two of three networks. Which either would require custom cabling or something dynamic like vlans. Either way required host configuration in the network that was dependent on the software that would be running on the host. If you can help it, you don't want networking in the way of the software that is running on the host. How did we solve this? We want back to the software teams and asked if we could just have each host have one NIC on one network. We were in the process of building out new datacenter design because of big changes in Amazon.com. In early 2004 Amazon moved Amazon.com from our Seattle datacenter to datacenters in Virginia. We used that change to clean up the network. We got rid of the dbnet and had just a website and corporate network. It was fairly easy when the hosts were ordered to know website or corporate, so this was not a big burden. This is an example of understanding the tradeoffs. The business originally asked for two NICs probably in support of better security, but that wasn't really true and it lead to more operational pain which lead to more outages. It's certainly easier to have this argument that it didn't give better security and just led to more problems after you have years of struggling to keep the network running. At this point it's easier to argue that the network needs less complexity.

After we had these things in place, then we could make the host network all L3. This also meant that each host network was almost the same, and that we could then automate configuration of all the ToR switches, which happened in 2005 or 2006

## Multicast

As I've [written about before](https://elegantnetwork.github.io/posts/Lessons-from-load-balancers-and-multicast/), Amazon was a huge multicast user in 2002-2006. This was because the of the explosion in what we called service oriented architecture, but is now called microservices. The main framework used for these microservices used a publish-subscribe methodology to communicate between most services. This was really cool, almost magic, but in the end it was a scaling disaster. It took a while for us to realize that abstracting traffic between services this way would not scale and was problematic. It helped that there were software problems with all this. When it became clear things couldn't work this way we very quickly switched to using LBs between services. This was again a time that we had to go back to software teams. However, in this case we had multiple outages and we were facing complete collapse as we rocket towards hard scaling cliffs in the hardware we had in the network.

## LB policy

Another example is from Load Balancers in the 200s, and while I can claim victory, in truth it's not as clear cut. Load Balancers are hard, and we in networking had the responsibility for them. Software teams wanted more sophisticated web traffic routing and we pushed back. Load balancers and very complicated and we didn't trust our ability to keep up with the policy changes and keep everything available. Eventually the software teams added a proxy layer in which they added software that they were in charge of to direct traffic as necessary. Over time this became more and more sophisticated in ways that I don't think we could have replicated on the Load Balancers. 

I know that other companies did (and probably still do) sophisticated traffic routing in their load balancers. We chose not to and it worked out well for us. Some part of that is that this breakout matches Amazon's culture much better and allowed the team that cared about traffic routing to own their destiny, and not have to rely on the networking team that had a lot of other things going on to worry about at the same time. 

## Tradeoffs of merchant silicon based routers

In 2009, several of us in AWS Networking were tasked with figuring out how to use merchant silicon based switches for for more than just ToR switches. In the end we came up with a large 3-tier Clos design to replace our aggregation network. As we were getting our heads around what this meant, several things became clear. We'd have hundreds (and then thousands) of these devices in a datacenter, very different from our tens and sometimes-low hundreds of devices than we had had before. Since the requirements for scale, including growth, became very clear, we had to change the way that we had been doing networking. 

We had to move to a configuration system that allowed very few options and reliably made every device the same. This set of tradeoffs meant that no longer could engineers just try new features, instead new things took a long time to test out, and then to get into our software systems. We moved to a configuration generation system that was correct by construction -- if it comes out of the generation system there is very little chance that it's broken. This kind of system is less flexible and takes longer for change but with the large number of devices that we had we thought it was the best way to keep everything consistent, which was required.

A controversial approach we took is that (almost) all changes to these devices are a full configuration replace and reboot of the device. Again, we thought through the tradeoffs. Rebooting every change adds time to some changes and it adds risk that the device or links won't come back up. It also requires software to manage that whole process when you have that many devices. On the other hand, we had many less workflows to figure out and program and we didn't have to work through all the bugs in NOSes for when their state changes. I think this turned out to be a very good decision, but it was mine, so I'm biased. However, the tradeoffs did cost us, it wasn't all good with no downside.

## Virtual Networking

In the first part of this post there is a section about remove L2. We made a consistent L3 network. Over time, and especially because of EC2 we had to add network virtualization back into the network. Because operating and scaling networking is hard, we insisted that the virtual networking happen in the host. When EC2 first launched (I think it's now called EC2 classis), the networking was simplistic, with no ability to choose your own addresses, but you could isolate your network from others. That was replaced with VPC, which is a much more sophisticated multi-tenant network virtualization. None of this was on the routers, so that we could deal with the scale of possibly millions of tenants with many millions of hosts. This means that the physical network doesn't know anything about the virtual network and can scale independently, which was necessary for AWS. Of course the tradeoff is a lot of sophisticated software on the hosts. I wish this option was more easily available for more networks, I'm no fan of EVPN in datacenters, but there's not a lot of options if you need virtual networking in your datacenter.

## Adding BGP to regions

Sometimes you have to add complexity, maybe not because of being easier to operate, but because of availability.

Amazon and AWS have multiple datacenters (availability zones) per region. Originally, in 2004 when we first did two datacenters in a region for Amazon.com, these were both pretty small and so we just continued to use OSPF in and between these datacenters. This grew over time. Between regions we used BGP, but instead a region was all OSPF. This was easier to operate, but didn't give us the isolation that we needed and had promised to customers. We had bugs in OSPF that then broke the whole region. We made mistakes in changes and broke the whole region. We then decided we had to add BGP and it's more sophisticated policy between the availability zones. This was a looong and complicated process because by this time there was a lot going on and we couldn't turn off Amazon.com and AWS for a couple days while we completely move to BGP.
## Conclusion

There are usually reasons, sometimes even good reasons for complexity in networks. As an engineer you need to think through the implication of those tradeoffs. You need to understand enough of what's going on on the hosts to know where/how the tradeoffs can be made. As your network and requirements change over time, think about how to make things simpler. As you notice what breaks, what is hard to change, what pages you in the middle of the night, think about how to make the network simpler.

Not all lessons from here are applicable to all people. First off, of course, L2 is not very highly used anymore for all the reasons I talk about here. Second, at Amazon keeping up with business scale became the most important thing, and we had software teams to throw at some of the problems, so we had to make tradeoffs that aren't worth it for other people. 
